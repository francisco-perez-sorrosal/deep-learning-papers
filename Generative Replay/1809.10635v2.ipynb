{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "cm.update(\n",
    "    \"rise\",\n",
    "    {\n",
    "        \"theme\": None,\n",
    "        \"transition\": None,\n",
    "        \"start_slideshow_at\": \"selected\",\n",
    "        \"leap_motion\": {\n",
    "            \"naturalSwipe\"  : True,     # Invert swipe gestures\n",
    "            \"pointerOpacity\": 0.5,      # Set pointer opacity to 0.5\n",
    "            \"pointerColor\"  : \"#d80000\" # Red pointer\"nat.png\"\n",
    "        },\n",
    "        \"header\": \"<h3>Francisco Perez-Sorrosal</h3>\",\n",
    "        \"footer\": \"<h3>Machine Learning/Deep Learning</h3>\",\n",
    "        \"scroll\": True,\n",
    "        \"enable_chalkboard\": True\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.emojize('Presenting stuff is easy!!! :thumbs_up:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Emojis http://getemoji.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain-inspired Replay for Continual Learning with ArtiÔ¨Åcial Neural Networks\n",
    "[Nature (13 August 2020)](https://www.nature.com/articles/s41467-020-17866-2)\n",
    "## Gido M. van de Ven & Hava T. Siegelmann & Andreas S. Tolias\n",
    "\n",
    "\n",
    "### With Excerps from:\n",
    "\n",
    "#### \"Three Scenarios for Continual Learning\" [https://arxiv.org/pdf/1904.07734.pdf](https://arxiv.org/pdf/1904.07734.pdf)\n",
    "##### Gido M. van de Ven & Andreas S. Tolias\n",
    "#### \"Generative Replay with Feedback Connections as a General Strategy for Continual Learning\"  [https://arxiv.org/pdf/1809.10635v2.pdf](https://arxiv.org/pdf/1809.10635v2.pdf)\n",
    "##### Gido M. van de Ven & Andreas S. Tolias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Francisco Perez-Sorrosal | 7 Dec 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Context\n",
    "\n",
    "__Avoid Catastrophic Forgetting in Artificial Neural Networks__\n",
    "\n",
    "- As other papers in the field, authors cite seminal work from *McCloskey* and *Ratcliff* psychology-based papers from early 90's on the problems of the connectionist paradigm for avoiding the so-called *memory loss*:\n",
    "\n",
    "    *_\"New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.\"_*\n",
    "\n",
    "    -- McCloskey et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Relationships Among the Papers\n",
    "\n",
    "1. _Generative Replay with Feedback Connections as a General Strategy for Continual Learning_\n",
    "2. _Three Scenarios for Continual Learning_\n",
    "3. _Brain-inspired replay for continual learning with artiÔ¨Åcial neural networks_\n",
    "\n",
    "\n",
    "### Relationships:\n",
    "\n",
    "* 1 : Aims at describing a new *scalable generative replay* method to avoid catastrophic forgetting in lifelong learning\n",
    "* 1 $\\longrightarrow$ 2 : As part of it, the authors suggest a new framework for **fair evaluation of catastrophic forgetting** consisting in 3 different scenarios for Incremental Learning\n",
    "* 1 + 2 + extensions $\\longrightarrow$ 3  : Extends the *scalable generative replay* from 1. with additional SotA brain-research techniques in neuroscience and applied to the scenarios described in 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main Goal and Contributions\n",
    "\n",
    "__Naive Description of the Papers' Goal__: Avoid to always re-training models on the data of all classes so far, avoiding catastrophic forgetting at the same time.\n",
    "\n",
    "All along the three papers, the __main contributions__ are:\n",
    "\n",
    "üí° Make comparison of continual learning scenarios easier and more rigurous\n",
    "  - Identifies 3 distinct scenarios depending on if the task identity is provided at test time or not.\n",
    "\n",
    "üí° Compare recently proposed SotA methods on continual learning\n",
    "\n",
    "üí° Recognize Generative Replay (GR) [1] as the only SotA method competive enough in the 3 scenarios identified\n",
    "  - Choose GR as a reference base method where to build new techniques\n",
    "\n",
    "üí° Propose a new fast, scalable and competitive (performance-wise) GR approach that behaves well in the 3 scenarios identified\n",
    "\n",
    "üí° Extend the new GR approach proposed with recent advances from neurosciences applied to ANN\n",
    "\n",
    "üí° Generate new perspectives and hypotheses about the computational role and possible implementations of replay in the brain\n",
    "\n",
    "\n",
    "[1] Shin, H., Lee, J. K., Kim, J. & Kim, J. Continual learning with deep generative\n",
    "replay. In Advances in Neural Information Processing Systems (eds. Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S. & Garnett, R.) 2994‚Äì3003(Curran Associates, Inc., Long Beach, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continual Learning Scenarios\n",
    "\n",
    "\n",
    "## Overall Problem framed as Classification: An Example\n",
    "\n",
    "- Imagine an agent\n",
    "\n",
    "- The agent trained to learn a simple task **A**: _\"classify cats and dogs\"_\n",
    "\n",
    "- After the initial training on **A**, the agent is trained to learn another simple task **B**: _\"classify cows and horses\"_\n",
    "\n",
    "The problem is that, in this setup, there may be different expectations/assumptions/interpretations that can be done by researchers:\n",
    "\n",
    "1. On one hand, some may expect the agent to solve the exact classification tasks it was trained on\n",
    "2. However, distinguishing between classes from different learning episodes may seem also a rational expectation if the whole scenario is seen as two sequential events\n",
    "\n",
    "- __That is, naively we would expect that the agent should now also be able to distinguish between cats and cows__\n",
    "\n",
    "- The difference in the two expectations described above, it turns out to dramatically affect the difficulty of a continual learning problem, hence the __necessity to distinguish between -at least- these two scenarios__\n",
    "\n",
    "- Most of the __SotA ML algorithms for CL fail in the second scenario__ even on seemingly simple toy examples\n",
    " - Only Generative Replay performs well on the second scenario\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Continual Learning Scenarios\n",
    "\n",
    "üí° **KEY**: *WHETHER OR NOT* the model is required to identify the identity of the task it has to solve at test time\n",
    "\n",
    "  - e.g. In N. Masse et al.'s \"Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization\", which assumes task identity is always available, it is reported a great improvement over SotA.\n",
    "\n",
    "\n",
    "##  Scenarios:\n",
    " \n",
    " 1. Models are **always** informed about which task is solving (Task-IL)\n",
    " \n",
    " \n",
    " 2. Models **do not know** the task identity at test time (domain-incremental learning, Domain-IL)\n",
    "   - However, the model do not need to infer the task, only solve it\n",
    " \n",
    " \n",
    " 3. Models need to both, **solve each task seen and infer which task** are currently evaluating (class-incremental learning, Class-IL)\n",
    " \n",
    " ![3Scenarios for CL](images/3scenarios.png)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Three Scenarios for Continual Learning (pdf)](https://arxiv.org/pdf/1904.07734.pdf)\n",
    "\n",
    "## Gido M. van de Ven & Andreas S. Tolias\n",
    "\n",
    "### Neurips 2019\n",
    "\n",
    "\n",
    "Extended description of the scenarios addressing the following:\n",
    "\n",
    "üí° The scenarios for Continual Learning Analisys\n",
    "\n",
    "üí° Strategies for Continual Learning\n",
    "\n",
    "üí° Experiments based on the previous scenarios presented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Scenarios\n",
    "\n",
    "## Task-IL\n",
    "\n",
    "- The easiest continual learning scenario\n",
    "- Models are always informed about which task needs to be performed\n",
    "- It's possible to train models with task-specific components \n",
    "- Typical network architecture in this scenario has a ‚Äúmulti-headed‚Äù output layer\n",
    "\n",
    "## Domain-IL\n",
    "\n",
    "- Task identity is not available at test time\n",
    "- BUT... models however only need to **solve the task**, not infer the current task\n",
    "- Task structure is always the same, **BUT** the input-distribution changes\n",
    "- Example: agents that have to survive in different envs without previous knowledge of the environment itself\n",
    "\n",
    "## Class-IL\n",
    "\n",
    "- Models have to solve each task seen AND infer the new current task\n",
    "- Name refers to the fact that the model has to learn to identify new classes of objects like infants do in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Single Headed vs Multi-Headed Methods\n",
    "\n",
    "Classical view of the continual learning:\n",
    "\n",
    "üí° A multi-headed layout requires task identity to be known (Task-IL)\n",
    "\n",
    "üí° A single-headed layout **DOES NOT** requires the task identity to be known (Domain-IL/Class-IL)\n",
    "\n",
    "This distinction is done **based on the architectural layout of a network‚Äôs output layer**. \n",
    "\n",
    "- **BUT** despite using a separate output layer for each task, it is the most common way to have specific task identity information, **it is not the only way**\n",
    "\n",
    "- For a single-headed layout might by\n",
    "itself not require task identity to be known, it is still possible for the model to use task identity in\n",
    "other way (See [1])\n",
    "\n",
    "## Major differences/advantages of the 3 scenarios proposal vs the Single/Multi Headed classical view...\n",
    "\n",
    "1. The scenarios proposed in the paper reflect more generally the conditions under which a model is evaluated.\n",
    "\n",
    "2. The scenarios extend upon the multi-headed vs single-headed split by recognizing that:\n",
    "    - when task identity is not provided, there is a further distinction depending on whether the network is explicitly required to infer task identity\n",
    "    - the two scenarios resulting from this additional split substantially differ in difficult\n",
    "\n",
    "[1] *Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization*, by N. Masse et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example Task Protocols\n",
    "\n",
    "üí° Goal is twofold: \n",
    "\n",
    "1. Show that any task protocol can be performed according to each scenario\n",
    "  -  Exercised through two different task protocols for all three scenarios\n",
    "2. Demonstrate the difference between the three continual learning scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Protocols\n",
    "\n",
    "### Sequentially learning to classify MNIST-digits\n",
    "\n",
    "\n",
    "![Split MNIST](images/splitmnist.png)\n",
    "\n",
    "\n",
    "\n",
    "Demonstrates:\n",
    "\n",
    "\n",
    "\n",
    "1. The Task-IL scenario\n",
    "  - It is sometimes referred to as ‚Äòmulti-headed split MNIST‚Äô\n",
    "  \n",
    "\n",
    "2. The Class-IL scenario\n",
    "  - It is referred to as ‚Äòsingle-headed split MNIST‚Äô\n",
    "\n",
    "\n",
    "3. It could also be performed under the Domain-IL scenario\n",
    "\n",
    "\n",
    "![Split MNIST](images/splitmnistscenarios.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Permuted MNIST\n",
    "\n",
    "- Each task involves classifying all ten MNIST-digits but with a different permutation applied to the pixels for every new task (Figure 2). \n",
    "\n",
    "![Permuted MNIST](images/permutedmnist.png)\n",
    "\n",
    "\n",
    "Demonstrates:\n",
    "\n",
    "\n",
    "1. Naturally the Domain-IL scenario\n",
    "2. But also it can be performed according to the other scenarios too.\n",
    "\n",
    "![Permuted MNIST](images/permutedmnistscenarios.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Task Boundaries\n",
    "\n",
    "\n",
    "## Important Assumption: \n",
    "\n",
    "**In training, there are clear and well-defined boundaries between the tasks to be learned**\n",
    "\n",
    "* **Objective of well defined boundaries?**\n",
    "  - Make the continual learning process more structured\n",
    "  - Without that structure, the scenarios described become blurry\n",
    "\n",
    "## Implications:\n",
    "\n",
    "* Among others, **training with randomly-sampled minibatches** and **multiple passes over each task‚Äôs training data** are no longer possible. \n",
    "* Without well-defined task-boundaries, see **Task agnostic continual learning using online variational bayes** by *C. Zeno et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methods Compared: Task-specific\n",
    "\n",
    "## [XdG, Context Dependent Gating](https://www.pnas.org/content/115/44/E10467)\n",
    "\n",
    "- Inherited from the [Context-dependent gating](https://en.wikipedia.org/wiki/Synaptic_gating) concept from neurosciences\n",
    "- Task switching disinhibits nonoverlapping sets of sparse dendritic branches\n",
    "    - Intersection of changes ~ 0; Minimal interference of synaptic changes for one task with synaptic changes that occurred for previous tasks\n",
    "- Simplified version of this XdG\n",
    "    - Algorithm sends an additional signal unique for each task, which is projected onto all hidden neurons\n",
    "    - X% of the units in each hidden layer was fully gated (i.e., their activations set to zero)\n",
    "        - X treated as a hyperparameter (set by grid search)\n",
    "    - In summary: **Each node in the ANN (randomly and a priori) is assigned to be involved in each task**\n",
    "- Small computational impact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methods Compared: Regularization-based\n",
    "\n",
    "- Add a regularization term to the loss\n",
    "- The impact of regularization is controlled by a hyperparameter $Œª$\n",
    "- $L_{total} = L_{current} + Œª * L_{regularization}$\n",
    "\n",
    "## [EWC, Elastic Weight Consolidation](https://arxiv.org/abs/1612.00796)\n",
    "\n",
    "- Main difference with XdG:\n",
    "    - Train a different part of the network for each task, but always use the entire network for execution\n",
    "- HOW? \n",
    "    - Regularize the ANN params while training each new task\n",
    "    - For all params in the ANN it is estimated if they are for the previously learned tasks\n",
    "        - Depending on this, they are penalized for future changes\n",
    "        - This is equivalent to reduce the learning process in some parts of the network that are supposedly \"remembering\" the previous tasks\n",
    "- Suitable for reinforcement learning scenarios\n",
    "- Favours more the initial tasks\n",
    "\n",
    "## [online EWC](https://arxiv.org/pdf/1805.06370.pdf)\n",
    "\n",
    "- **EWC criticism** from [this paper](https://arxiv.org/abs/1712.03847): \n",
    "    -  For two tasks, EWC is ~ a diagonalized Laplace approximation, with a new hyperparameter $Œª_A$ that tries to signify the task importance\n",
    "    - Basically the critizism states of EWC that, when more than two tasks are considered, the quadratic penalties in EWC are inconsistent with this derivation (grows linearly with the number of tasks) and might lead to double-counting data from earlier tasks.\n",
    "\n",
    "- Online EWC is a modification of EWC focused on scalability\n",
    "- Favours more the most recent past\n",
    "- Tries to tame growth of the computational cost of the regularization\n",
    "\n",
    "\n",
    "## [SI, Synaptic Intelligence](https://arxiv.org/abs/1703.04200)\n",
    "\n",
    "- **NOT DIRECTLY related to biological mechanisms, just observations:** \"While we make no claim that biological synapses behave like the intelligent synapses of our model, a wealth of experimental data in neurobiology suggests that biological synapses act in much more complex ways than the artificial scalar synapses that dominate current machine learning models. *In essence, whether synaptic changes occur, and whether they are made permanent, or left to ultimately decay, can be controlled by many different biological factors.*\"\n",
    "\n",
    "- Similar to EWC in the sense of training part of the ANN per task, but fully use the ANN for execution\n",
    "    - Paper states: \"The regularization penalty is similar to EWC as recently introduced by Kirkpatrick et al. (2017).\"\n",
    "- On the contrary to EWC, SI computes the per-synapse consolidation strength:\n",
    "    - Online (Similar to what is proposed in online EWC)\n",
    "    - Over the entire learning trajectory in parameter space\n",
    "- Conjeturates that individual synapses not correspond simply to single scalar synaptic weights, and imnplements them to behave as a higher dimensional dynamical systems\n",
    "- These high dimensional states of these SI synapses makes possible:\n",
    "    - Accumulate task relevant information more efficiently during training\n",
    "    - Retain a memory of previous parameter values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Methods Compared: Replay-based methods\n",
    "\n",
    "- Loss function consists of:\n",
    "    - A term for the data of the current task and...\n",
    "    - A term for the replayed data\n",
    "\n",
    "- The loss for current/replayed data can be weighted according to the # of tasks the model has been trained on so far\n",
    "    - $L_{total} = 1/N_{tasks so far} * L_{current} + (1 - 1/N_{tasks so far}) * L_{replay}$\n",
    "    \n",
    "\n",
    "## [LwF, Learning Without Forgetting](https://arxiv.org/abs/1606.09282)\n",
    "\n",
    "- Considered by the authors a replay-based method\n",
    "- Train a model M1 for task, A with labeled data -> Label input data for task B with M1 -> Use the resulting input-target pairs as pseudo-data for task B.\n",
    "- Inputs to be replayed labeled with ‚Äúhard targets‚Äù + ‚Äúsoft targets‚Äù\n",
    "    - ‚Äúhard targets‚Äù - the most likely category according to the previous tasks‚Äô model\n",
    "    - ‚Äúsoft targets‚Äù - previous tasks‚Äô model predicted probabilities for all target classes\n",
    "- Goal for the replayed data: to match the probabilities predicted by the model being trained to these target probabilities. Similar to distillation.\n",
    "\n",
    "![LWF1](images/lwf1.png)![LWF2](images/lwf2.png)![LWF3](images/lwf3.png)\n",
    "\n",
    "\n",
    "## [DGR (Deep Generative Replay or just GR)](http://arxiv.org/abs/1705.08690)\n",
    "- Two models:\n",
    "    - **Generative model** -- creates data to be replayed, sequentially trained on all tasks (according to each task's input data distribution)\n",
    "    - **Main model** -- used for evaluate task performance (classification, etc.)\n",
    "- Input samples were paired with ‚Äúhard targets‚Äù provided by the main model\n",
    "\n",
    "\n",
    "## [DGR+distill](https://arxiv.org/abs/1802.00853)\n",
    "\n",
    "- Combination of LwF and DGR\n",
    "- Separate generative model trained to generate images to be replayed, but these were then paired with soft targets (LwF) instead of hard targets (DGR)\n",
    "\n",
    "\n",
    "## [iCaRL](https://arxiv.org/abs/1611.07725)\n",
    "\n",
    "- Only the training data for a small number of classes has to be present at the same time and new classes can be added progressively\n",
    "- Can learn many classes incrementally over a long period of time where other strategies quickly fail\n",
    "- It can only be applied in the Class-IL scenario\n",
    "  - However, two components of iCaRL are suitable for all scenarios:\n",
    "    - the use of exemplars for classification \n",
    "    - and the replay of stored data during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "__Baselines__:\n",
    "- None: Model sequentially trained on all tasks in the standard way (fine-tuning)\n",
    "  - Lower bound\n",
    "- Offline: Model trained using the data of all tasks so far (joint training,)\n",
    "  - Upper bound\n",
    "\n",
    "## Split MNIST\n",
    "\n",
    "![3 Scenarios Split MNIST](images/3smnist.png)\n",
    "\n",
    "## Permuted MNIST\n",
    "\n",
    "![3 Scenarios Permuted MNIST](images/3pmnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-Aways\n",
    "\n",
    "* Class-IL scenario (i.e., when task identity must be inferred), __only replay-based methods are capable of producing acceptable results__\n",
    "  - Replay might be an unavoidable tool\n",
    "\n",
    "* __Limitation of the current study__: MNIST-images are relatively easy to generate\n",
    "  - *Open question*: Will GR still be so successful for task protocols with more complex input dist.?\n",
    "\n",
    "* __Intuition__:\n",
    "  - Even if the quality of replayed samples is not perfect, they could still be very helpful\n",
    "  \n",
    "* Alternative/complement to replaying generated samples:\n",
    "  - Store examples from previous tasks and replay them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bonus: Continual Learning Methods Classification\n",
    "\n",
    "Source: [A continual learning survey: Defying forgetting in classification tasks (2020)](https://arxiv.org/pdf/1909.08383.pdf)\n",
    "\n",
    "![continual_learning_methods](images/contlearningmethods.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [\"Generative Replay with Feedback Connections as a General Strategy for Continual Learning\" (pdf)](https://arxiv.org/pdf/1809.10635v2.pdf)\n",
    "\n",
    "## Gido M. van de Ven & Andreas S. Tolias\n",
    "\n",
    "### Arxiv 2018\n",
    "\n",
    "\n",
    "Early version of the Nature paper, setting up the basis:\n",
    "\n",
    "üí° Presents also the 3 scenarios for Continual Learning Analisys\n",
    "\n",
    "üí° Focus on GR as the base where to build brain-inspired techniques\n",
    "\n",
    "üí° Focus mainly on RtF technique as a potential advantage\n",
    "\n",
    "üí° Results already shown the advantage of applying RtF over vanilla GR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Memory vs Generative Replay\n",
    "\n",
    "![Memory vs Generative Replay](images/memoryvsgenerativereplay.png)\n",
    "\n",
    "- GR is the only method capable of performing well in the Class-IL scenario without storing data\n",
    "- __GR drawback__: scaling it up to more challenging problems has been reported to be problematic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "## Split MNIST task protocol\n",
    "![Split MNIST](images/results_p3_smnist.png)\n",
    "\n",
    "\n",
    "### Task-IL & Class-IL (Nature)\n",
    "- Compared with the original paper, EWC methods are doing better here\n",
    "![Split MNIST (Nature)](images/results_p1_smnist.png)\n",
    "\n",
    "\n",
    "## Permuted MNIST task protocol\n",
    "\n",
    "![Permuted MNIST](images/results_p3_pmnist.png)\n",
    "\n",
    "### Domain-IL (Nature)\n",
    "![Permuted MNIST (Nature)](images/braininspiredeval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain-inspired modifications to GR\n",
    "\n",
    "### Current GR Approaches\n",
    "\n",
    "![Current GR approach](images/currentGR.png)\n",
    "\n",
    "### Motivation: Replay does not need to be perfect BUT also NOT LOW QUALITY\n",
    "  - __Simple/Naive approach__: Use recent progress in generative modelling with DNNs\n",
    "    - Drawbacks: Train those models is complex and expensive\n",
    "  - __Adopted approach__: Follow brain inspiration\n",
    "  \n",
    "### Brain inspired Techniques used\n",
    "\n",
    "![Nature GR approach](images/brainGR.png)\n",
    "\n",
    " 1. __Replay Through Feedback__\n",
    "  - Replay in brain originates in the hippocampus and propagates to the cortex\n",
    "  - This fact is ignored by current GR methods\n",
    "  - __Proposal__: merge the generator into the main model, adding generative backward/feedback connections\n",
    "  - Implemented as VAE with added softmax classification layer to the top layer of its encoder\n",
    "  \n",
    " 2. __Conditional Replay__\n",
    "  - Human brain has control over what memories are recalled, but VAEs don't\n",
    "  - __Proposal__: Allow the VAE to generate examples of a particular clas\n",
    "  - Implementation: the standard normal prior is replaced by a Gaussian mixture with a separate mode for each class\n",
    " \n",
    " 3. __Context Dependent Gating__\n",
    "  - In the human brain, contextual cues (e.g., odours, sounds) bias what memories are replayed\n",
    "  - XdG inhibits a different, randomly selected subset of neurons in each hidden layer depending the task.\n",
    "  - Approach not valid for Domain/Class-IL scenarios\n",
    "  - __Proposal__: Do gating based on internal context\n",
    "  - Implementation: The internal context conditioned on is the specific task/class to be generated or reconstructed during the generative backward pass.\n",
    "\n",
    " 4. __Internal replay__\n",
    "  - Human brain does not replay memories all the way down to the input level (e.g. you don't propagate a mental object till the retina)\n",
    "  - __Proposal__: Replay representations of previously learned classes not all the way to the input level (e.g., pixel level), but to replay them internally or at the 'hidden level'\n",
    "   - Avoid or allow only very limited changes to the first few layers that are not being replayed\n",
    "   - Consistent with observations in neuroscience experiments\n",
    "   \n",
    "### ML-inspired Techniques used\n",
    "\n",
    " 5. __Soft Labels (Embedded Distillation)__\n",
    "  - __Proposal__: Avoid to label generated data strictly using hard labels.\n",
    "  - Generated data is labeled with the predicted probabilities for all possible classes (soft labels)\n",
    "   - When the quality of the generated data is low, might be harmful to label ambiguous inputs (e.g., that are in between two or more classes) as belonging to a single class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain-Inspired Techniques Evaluation\n",
    "\n",
    "## Context\n",
    "\n",
    "- Permuted MNIST\n",
    "- Domain-IL scenario (i.e., no task labels available at test time)\n",
    "- Reported is average test accuracy based on all permutations so far\n",
    "- Displayed are the means over 5 repetitions, shaded areas are ¬±1 SEM (Standard Error of the Mean)\n",
    "\n",
    "![Brain Inspired Evaluation](images/braininspiredeval.png)\n",
    "\n",
    "## Take Aways\n",
    "\n",
    "1. GR outperforms SI for the first 10 tasks __but its performance rapidly degrades after ~15 tasks__\n",
    "2. Brain-inspired replay outperfors the already strong performance of SI (__achieving SotA performance__)\n",
    "3. Combining BI-R with SI results in a further boost in performance\n",
    "4. LwF performs badly on this task protocol because between tasks, the inputs are completely uncorrelated\n",
    "\n",
    "## Context\n",
    "\n",
    "- CIFAR 100\n",
    "- Task IL: Choice only between classes within given task\n",
    "- Class IL: Choice between all classes seen so far\n",
    "\n",
    "![Brain Inspired CIFAR](images/braininspiredCIFAR.png)\n",
    "\n",
    "## Take Aways\n",
    "\n",
    "- Task IL scenario\n",
    " 1. BI-R outperforms EWC, SI and LwF\n",
    " 2. BI-R almost fully mitigated catastrophic forgetting \n",
    "\n",
    "- Class-IL scenario\n",
    " 1. BI-R outperforms the other methods\n",
    " 2. Performance still remained substantially under the ‚Äòupper bound‚Äô\n",
    " 3. However, BI-R is the best method without storing data\n",
    " 4. Combining BI-R with SI results in a further boost in performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Addition Ablation Experiments\n",
    "\n",
    "- Standard GR with individual modifications added ('+', left)\n",
    "- BI-R with individual modifications removed ('‚àí', right)\n",
    "- Mean results over 5 (permuted MNIST) or 10 (CIFAR-100) repetitions\n",
    "- Dotted grey lines indicate chance level\n",
    "- Solid black lines show performance when the base network is trained only on the final task/episode \n",
    "- Techniques:\n",
    " - rtf replay-through-feedback\n",
    " - con conditional replay\n",
    " - gat gating based on internal context\n",
    " - int internal replay\n",
    " - dis distillation\n",
    "\n",
    "# Permuted MNIST\n",
    "\n",
    "![Permuted MNIST](images/ablationpmnist.png)\n",
    "\n",
    "- The gain in performance obtained by combining all components is larger than the sum of the effects of adding each of them in isolation\n",
    "- None of the individual modifications were sufficient to achieve the performance of BI-R\n",
    " - while all of them (with the exception of RtF) were necessary\n",
    "\n",
    "# CIFAR-100 Task-IL\n",
    "\n",
    "![CIFAR-100 Task-IL](images/ablationcifar100t.png)\n",
    "\n",
    "- None of the individual components were necessary for the Task-IL scenario\n",
    " - Preventing catastrophic forgetting is easier here\n",
    "\n",
    "\n",
    "# CIFAR-100 Class-IL\n",
    "\n",
    "- __Pay attention at the scale of the Y-Axis here__\n",
    "\n",
    "![CIFAR-100 Class-IL](images/ablationcifar100c.png)\n",
    "\n",
    "- The gain in performance obtained by combining all components is larger than the sum of the effects of adding each of them in isolation\n",
    "- None of the individual modifications were sufficient to achieve the performance of BI-R\n",
    " - while all of them (with the exception of RtF) were necessary\n",
    "\n",
    "\n",
    "## Take Aways\n",
    "\n",
    "- __Most influential__: _Internal replay_. Effect on performance\n",
    "- __RtF increases efficiency (i.e., removing the need for a separate generative model) without substantially hurting performance__\n",
    " - Best overall performance was in fact obtained withOUT RtF <- I think this is a typo in the Nature paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary and Take Aways\n",
    "\n",
    "üí°Biological NNs are superior to ANNs counterparts when it comes to continual learning\n",
    " - The brain has inspired recent attempts to alleviate catastrophic forgetting in ANNs:\n",
    "  - Regularization-based methods such as EWC and SI model the complexity of biological synapses\n",
    "  - XtG models brain's ability to process stimuli differently depending on context (Helps in Task-IL scenarios)\n",
    " - Successful for scenarios in which tasks must be learned incrementally __BUT__, those methods are unable to incrementally learn new classes\n",
    " \n",
    "üí°Authors show how simple, easy-to-implement and efficient brain-inspired modifications can enable GR to successfully scale to problems with many tasks or complex inputs\n",
    "\n",
    "üí°GR facilitates incrementally learning a generative model\n",
    " - and the __brain-inspired modifications improve the quality__ of the learned generative model\n",
    " \n",
    "üí°In semi/unsupervised settings, the conditional replay and the gating based on internal context components would need to be modified as their current implementation depends on the availability of class labels during training\n",
    "\n",
    "üí°Drawbacks:\n",
    " 1. Other input modalities:\n",
    "  - Require different pre-processing layers, and it remains to be confirmed whether replaying internal representations will work with those\n",
    "  - Work to be done on this\n",
    "  - Analogous to the separate sensory processing areas in the brain\n",
    " 2. Rigid, pre-trained convolutional layers likely restrict the ability of the model to learn out-of-distribution inputs\n",
    "  - e.g., images without natural image statistics\n",
    "  - Similar for the brain though\n",
    " 3. CL performance was only quantified by the average accuracy over all tasks or classes seen so far\n",
    "  - This is a measure that mainly reflects the extent to which a method suffers from catastrophic forgetting\n",
    "  - Critical aspects of CL such as forward and backward transfer or compressability were not addressed\n",
    "   - Specially affects Task-IL scenarios (where catastrophic forgetting can be prevented by simply training a different network for each task to be learned)\n",
    " 4. Preventing catastrophic forgetting in Class-IL is still an unsolved problem\n",
    "  - Justifies authors' focus on the average accuracy measure\n",
    "  \n",
    "  \n",
    "üí° Generate new perspectives and hypotheses about the computational role and possible implementations of replay in the brain\n",
    " - Provides evidence that replay might indeed be a feasible way for the brain to combat catastrophic forgetting\n",
    " - Postulates replay in the brain to be a generative process\n",
    " - Representations replayed in the brain do not directly reflect experiences, but that they might be samples from a learned model of the world [1,2,3,4]\n",
    " - Current models assume that examples for all categories are either observed together or that they can be directly stored in memory (e.g., exemplar-, prototype- and rule- based models all rely on this assumption)\n",
    "  - GR could be a biologically plausible way to extend these models to the more natural case in which the different categories to be learned are only available sequentially\n",
    " - Missing aspects of the proposed model:\n",
    "  - *Temporal structure*: replay-events in the brain consist of sequences of neuronal activity that reflect the temporal order of the actual experiences\n",
    "\n",
    "üí° Why GR is so much more effective for Class-IL than regularization-based methods such as EWC and SI\n",
    " - How memory is stored?\n",
    "  - GR in the function or output space of the network\n",
    "  - Reg-based store and maintain the memory of previous classes entirely in the parameter space of the network\n",
    "   - Task-IL the memory to be stored is simpler, because only the features important for the specific task learned at that time need to be remembered  \n",
    "   - Class-IL this might be challenging, since all information about previous classes must be kept, as it is unknown what the future classes will be like\n",
    "  - Reg-based methods can't solve Class-IL by themselves __BUT__ provide a unique contribution when combined with GR\n",
    "   - __Hypothesys__: maintaining memories in function space and maintaining them in parameter space each come with their own, separate challenges:\n",
    "    - GR: the challenge is to learn a generative network that captures enough of the essence of the previous tasks/classes\n",
    "    - Reg-based: the challenge is to correctly assign credit to the parameters of the network\n",
    "   - Neurosciences observation: regularization (or metaplasticity) and replay are complementary mechanisms, which is consistent with empirical observations that the brain uses both strategies side by side to protect its memories [5]\n",
    " \n",
    "[1] Gupta, A. S., van der Meer, M. A., Touretzky, D. S. & Redish, A. D. Hippocampal replay is not a simple function of experience. Neuron 65, 695‚Äì705 (2010).\n",
    "\n",
    "[2] OÃÅlafsdoÃÅttir, H. F., Barry, C., Saleem, A. B., Hassabis, D. & Spiers, H. J. Hippocampal place cells construct reward related sequences through unexplored space. Elife 4, e06063 (2015).\n",
    "\n",
    "[3] Liu, Y., Dolan, R. J., Kurth-Nelson, Z. & Behrens, T. E. Human replay spontaneously reorganizes experience. Cell 178, 640‚Äì652 (2019).\n",
    "\n",
    "[4] Foster, D. J. Replay comes of age. Annu. Rev. Neurosci. 40, 581‚Äì602 (2017).\n",
    "\n",
    "[5] Genzel, L. & Wixted, J. T. Cellular and systems consolidation of declarative memory in Cognitive Neuroscience of Memory Consolidation (eds. Axmacher, N. & Rasch, B.) 3‚Äì16 (Springer, Switzerland, 2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reflections/Open Questions\n",
    "\n",
    "- I share with Terrence J. Sejnowski the following ‚ÄúMy belief in [artificial] neural networks was based on my intuition that if nature had solved this problems [vision, speech & language,] we should be able to learn from nature how to solve them, too.‚Äù\n",
    "\n",
    "- The paper does not compare their method with [Fearnet (Ronald Kemker and Christopher Kanan) in ICLR 2018](https://openreview.net/forum?id=SJ1Xmf-Rb), a generative model (that is memory efficient) that does not store previous examples, which apparently was getting SotA performance at incremental class learning on CIFAR-100\n",
    "\n",
    "- Fun Fact: None of the latest two big surveys about Continual Learning includes any reference to these authors:\n",
    "\n",
    "    - [A continual learning survey: Defying forgetting in classification tasks (2020)](https://arxiv.org/pdf/1909.08383.pdf)\n",
    "    - [Continual Lifelong Learning with Neural Networks: A Review (2019)](https://arxiv.org/pdf/1802.07569.pdf)\n",
    "\n",
    "\n",
    "- Nice set of papers appliying some of the recent advances in neuroscience/learning sciences\n",
    "    - In particular episodic memory and brain structures involved (hyppocampus and neocortex)\n",
    "    - Show how the current methods applying generative replay do not mimic exactly the real communication happening in the brain when learning\n",
    "    - Apparently [great source code](https://github.com/GMvandeVen/continual-learning) available\n",
    "\n",
    "\n",
    "- Can generative replay (or a similar technique) be applied to Transformer-based models?\n",
    "\n",
    "    - Probably language learning/processing is using a different mechanism in the brain [0]. Investigate brain-related structures to language. Summary sentence:\n",
    "    \"Our words are bound by an invisible grammar which is embedded in the brain.\" - Jonah Lehrer, in Proust Was a Neuroscientist.\n",
    "    - In particular to:\n",
    "        - Incremental tasks in text classification\n",
    "        - Hierarchical text classification tasks when adding new categories\n",
    "    - Would it be really necessary to have an extra generator? \n",
    "        - Can be based on the mere sampling of a small set of previous tasks examples only (e.g. applying zero-shot learning techniques)? \n",
    "        - Or maybe a can be expanded with a masked language model like ROBERTA to generate stuff automatically, in a similar way as some of the test cases generation described in [Beyond Accuracy: Behavioral Testing of NLP models with CheckList](https://arxiv.org/abs/2005.04118)\n",
    "\n",
    " \n",
    "- Is the controversy introduced in [1] when saying that generative replay _shifts_ the catastrophic forgetting problem to the training of the generative model_ really overcomed by this work? Or this paper shows that is not true and that a small amount of good enough replay generated by the model itself avoids most of the catastrophic forgetting occurring in the different scenarios?\n",
    "\n",
    "- I aggree, as it is also stated in [3], that \"In essence, in machine learning, in addition to adding depth to our networks, we may need to add intelligence to our synapses.\"\n",
    "\n",
    "- Observing and learn about how the brain works and translate the advances of neuroscience/learning science/psicology/psychiatry into the machine learning field has been proven very effective, specially in the last few years where computing power and data has grown almost exponentially. However, may obsesively trying mimic how the brain works, limit/shadow the potential of what additional artificial structures can add/complement to enhance the learning capabilities of current algorithms/techniques? As several theories proposed -See R. Kurtzweil for example-, maybe the changes we should pursue to advance the capacity of artificial \"brains\", should be similar to the structural changes caused by the evolution of neocortex [2] with regard to other species, and specially inside the primates.\n",
    "\n",
    "\n",
    "\n",
    "[0] Angela D. Friederici. [The Brain Basis of Language Processing: From Structure to Function (2011)](https://journals.physiology.org/doi/full/10.1152/physrev.00006.2011)\n",
    "\n",
    "[1] Jonathan Schwarz, Jelena Luketina, Wojciech M Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. [Progress & compress: A scalable framework for continual learning (ICLM 2020)](https://arxiv.org/pdf/1805.06370.pdf)\n",
    "\n",
    "[2] Jon H. Kaas (Prog Brain Res. 2012 ; 195: 91‚Äì102. doi:10.1016/B978-0-444-53860-4.00005-20) [The evolution of neocortex in primates](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3787901/pdf/nihms516054.pdf)\n",
    "\n",
    "[3] Friedemann Zenke, Ben Poole, Surya Ganguli. [Continual Learning Through Synaptic Intelligence](https://arxiv.org/abs/1703.04200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PDF Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"Ven et al. - 2020 - Brain-inspired replay for continual learning with.pdf\", width=1500, height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "IFrame(\"Generative replay with feedback connections as a general strategy for continual learning.pdf\", width=1500, height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "IFrame(\"Three Scenarios for Continual Learning.pdf\", width=1500, height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "autolaunch": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
